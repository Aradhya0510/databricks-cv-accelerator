# Databricks notebook source
# COMMAND ----------

# MAGIC %md
# MAGIC # 06. Model Monitoring and Drift Detection
# MAGIC 
# MAGIC This notebook demonstrates how to:
# MAGIC 1. Enable model monitoring for a Databricks Model Serving endpoint
# MAGIC 2. Log prediction and ground truth events for monitoring
# MAGIC 3. Analyze drift metrics and endpoint health
# MAGIC 4. Set up alerting for model performance and data drift
# MAGIC 5. Query logs and metrics using Databricks SQL
# MAGIC 6. Integrate with external alerting systems
# MAGIC 
# MAGIC ## Overview
# MAGIC 
# MAGIC **Model Monitoring Goals:**
# MAGIC - Detect data and prediction drift
# MAGIC - Monitor endpoint health and latency
# MAGIC - Track model performance over time
# MAGIC - Enable alerting for anomalies and failures
# MAGIC 
# MAGIC ### Key Monitoring Concepts:
# MAGIC - **Data Drift**: Changes in input data distribution
# MAGIC - **Prediction Drift**: Changes in model output patterns
# MAGIC - **Performance Monitoring**: Accuracy, latency, and throughput tracking
# MAGIC - **Health Monitoring**: Endpoint availability and error rates
# MAGIC - **Alerting**: Automated notifications for issues
# MAGIC 
# MAGIC ## What This Notebook Does
# MAGIC 
# MAGIC 1. **Enable Monitoring**: Set up monitoring for deployed endpoints
# MAGIC 2. **Log Events**: Log prediction and ground truth events
# MAGIC 3. **Analyze Metrics**: Query and analyze monitoring data
# MAGIC 4. **Set Up Alerting**: Configure alerts for drift and failures
# MAGIC 5. **Generate Reports**: Create monitoring dashboards and reports
# MAGIC 6. **Integration**: Connect with external monitoring systems
# MAGIC 
# MAGIC ---

# COMMAND ----------

# (Databricks only) Install requirements and restart Python if running interactively
# %pip install -r "../requirements.txt"
# dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Import Dependencies and Load Configuration

# COMMAND ----------

import sys
import os
import json
import requests
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path

# Add the src directory to Python path
sys.path.append('/Workspace/Repos/your-repo/Databricks_CV_ref/src')

from config import load_config, get_default_config
from utils.logging import create_databricks_logger

# Load configuration from previous notebooks
CATALOG = "your_catalog"
SCHEMA = "your_schema"  
VOLUME = "your_volume"
PROJECT_PATH = "cv_detr_training"

BASE_VOLUME_PATH = f"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{PROJECT_PATH}"
CONFIG_PATH = f"{BASE_VOLUME_PATH}/configs/detection_detr_config.yaml"

# Set up volume directories
MONITORING_RESULTS_DIR = f"{BASE_VOLUME_PATH}/monitoring_results"
MONITORING_LOGS_DIR = f"{BASE_VOLUME_PATH}/logs"

# Create directories
os.makedirs(MONITORING_RESULTS_DIR, exist_ok=True)
os.makedirs(MONITORING_LOGS_DIR, exist_ok=True)

print(f"üìÅ Volume directories created:")
print(f"   Monitoring Results: {MONITORING_RESULTS_DIR}")
print(f"   Monitoring Logs: {MONITORING_LOGS_DIR}")

# Load configuration
if os.path.exists(CONFIG_PATH):
    config = load_config(CONFIG_PATH)
    print(f"‚úÖ Configuration loaded successfully!")
else:
    print("‚ö†Ô∏è  Config file not found. Using default detection config.")
    config = get_default_config("detection")

# Initialize logging
logger = create_databricks_logger(
    name="model_monitoring",
    log_file=f"{MONITORING_LOGS_DIR}/monitoring.log"
)

# Model serving configuration
MODEL_NAME = config['model']['model_name']
ENDPOINT_NAME = f"detr-{MODEL_NAME.replace('/', '-')}"

print(f"‚úÖ Configuration loaded")
print(f"   Model: {MODEL_NAME}")
print(f"   Endpoint: {ENDPOINT_NAME}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Enable Model Monitoring on Endpoint

# COMMAND ----------

# MAGIC %md
# MAGIC ### Setup Model Monitoring

# COMMAND ----------

def setup_model_monitoring():
    """Enable model monitoring for the deployed endpoint."""
    
    print("üîß Setting up model monitoring...")
    
    try:
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import MonitoringConfig

        # Initialize workspace client
client = WorkspaceClient()

        # Configure monitoring
monitoring_config = MonitoringConfig(
    enabled=True,
    request_logging_enabled=True,
    response_logging_enabled=True,
            drift_threshold=0.1,  # 10% drift threshold
            performance_threshold=0.95,  # 95% performance threshold
            latency_threshold=1000  # 1 second latency threshold
)

        # Update endpoint with monitoring
try:
    client.serving_endpoints.update_config(
        name=ENDPOINT_NAME,
        monitoring=monitoring_config
    )
    print(f"‚úÖ Monitoring enabled for endpoint: {ENDPOINT_NAME}")
            print(f"   Drift threshold: {monitoring_config.drift_threshold}")
            print(f"   Performance threshold: {monitoring_config.performance_threshold}")
            print(f"   Latency threshold: {monitoring_config.latency_threshold}ms")
            
            return True
            
except Exception as e:
    print(f"‚ö†Ô∏è  Could not enable monitoring (may already be enabled): {e}")
            return False
            
    except Exception as e:
        print(f"‚ùå Monitoring setup failed: {e}")
        return False

monitoring_enabled = setup_model_monitoring()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Log Prediction and Ground Truth Events

# COMMAND ----------

# MAGIC %md
# MAGIC ### Log Prediction Events

# COMMAND ----------

def log_prediction_events(num_events=100):
    """Log prediction events to enable drift monitoring."""
    
    if not monitoring_enabled:
        print("‚ùå Monitoring not enabled")
        return False
    
    print(f"üìä Logging {num_events} prediction events...")
    
    try:
        # Get endpoint URL
        from databricks.sdk import WorkspaceClient
        client = WorkspaceClient()
        endpoint = client.serving_endpoints.get(ENDPOINT_NAME)
        endpoint_url = endpoint.state.config.inference_url
        
        # Create sample data for predictions
        sample_images = []
        for i in range(num_events):
            # Create realistic image data (simulating real-world usage)
            image = np.random.randint(0, 255, (800, 800, 3), dtype=np.uint8)
            sample_images.append(image.tolist())
        
        # Prepare request payload
        payload = {
            "dataframe_records": [
                {"image": image} for image in sample_images
            ]
        }
        
        # Make prediction requests
headers = {
            "Authorization": f"Bearer {dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()}",
    "Content-Type": "application/json"
}

        successful_requests = 0
        for i in range(0, num_events, 10):  # Process in batches of 10
            batch_payload = {
                "dataframe_records": payload["dataframe_records"][i:i+10]
}

            try:
                response = requests.post(
                    f"{endpoint_url}/invocations",
                    json=batch_payload,
                    headers=headers,
                    timeout=30
                )
                
if response.status_code == 200:
                    successful_requests += len(batch_payload["dataframe_records"])
                    print(f"   Batch {i//10 + 1}: {len(batch_payload['dataframe_records'])} predictions logged ‚úÖ")
else:
                    print(f"   Batch {i//10 + 1}: Failed ‚ùå ({response.status_code})")
                    
            except Exception as e:
                print(f"   Batch {i//10 + 1}: Error ‚ùå ({str(e)})")
        
        print(f"‚úÖ Prediction logging completed")
        print(f"   Successful requests: {successful_requests}/{num_events}")
        print(f"   Success rate: {successful_requests/num_events*100:.1f}%")
        
        return successful_requests > 0
        
    except Exception as e:
        print(f"‚ùå Prediction logging failed: {e}")
        return False

prediction_events_logged = log_prediction_events()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Log Ground Truth Events

# COMMAND ----------

def log_ground_truth_events():
    """Log ground truth events for drift detection."""
    
    if not monitoring_enabled:
        print("‚ùå Monitoring not enabled")
        return False
    
    print("üìä Logging ground truth events...")
    
    try:
        # Get monitoring API URL
        from databricks.sdk import WorkspaceClient
        client = WorkspaceClient()
        
        # Create sample ground truth data
        ground_truth_data = []
        for i in range(50):  # Log 50 ground truth events
            ground_truth = {
                "request_id": f"req-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{i:03d}",
                "ground_truth": {
                    "num_objects": np.random.randint(1, 10),
                    "avg_confidence": np.random.uniform(0.6, 0.95),
                    "detection_classes": np.random.choice(range(80), size=np.random.randint(1, 5), replace=False).tolist()
                }
            }
            ground_truth_data.append(ground_truth)
        
        # Log ground truth events
        successful_logs = 0
        for gt_event in ground_truth_data:
            try:
                # In a real scenario, you would use the actual monitoring API
                # For demonstration, we'll simulate the logging
                print(f"   Logged ground truth for request: {gt_event['request_id']}")
                successful_logs += 1
                
            except Exception as e:
                print(f"   Failed to log ground truth for {gt_event['request_id']}: {e}")
        
        print(f"‚úÖ Ground truth logging completed")
        print(f"   Successful logs: {successful_logs}/{len(ground_truth_data)}")
        print(f"   Success rate: {successful_logs/len(ground_truth_data)*100:.1f}%")
        
        return successful_logs > 0
        
    except Exception as e:
        print(f"‚ùå Ground truth logging failed: {e}")
        return False

ground_truth_logged = log_ground_truth_events()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Analyze Drift Metrics and Endpoint Health

# COMMAND ----------

# MAGIC %md
# MAGIC ### Query Endpoint Metrics

# COMMAND ----------

def analyze_endpoint_metrics():
    """Analyze endpoint metrics and health."""
    
    print("üìä Analyzing endpoint metrics...")
    
    try:
        from databricks.sdk import WorkspaceClient
        client = WorkspaceClient()
        
        # Get endpoint information
endpoint = client.serving_endpoints.get(ENDPOINT_NAME)

        print(f"‚úÖ Endpoint Analysis:")
        print(f"   Endpoint: {endpoint.name}")
        print(f"   State: {endpoint.state.ready}")
        print(f"   Health: {getattr(endpoint.state, 'health', 'Unknown')}")

        # Analyze served models
        if hasattr(endpoint.state, 'served_models'):
            for model in endpoint.state.served_models:
                print(f"   Model: {model.name}")
                print(f"   Version: {model.version}")
                print(f"   Status: {model.status}")
        
        # Check for monitoring metrics
        if hasattr(endpoint.state, 'monitoring_metrics'):
            print(f"\nüìà Monitoring Metrics:")
            metrics = endpoint.state.monitoring_metrics
            for metric_name, metric_value in metrics.items():
                print(f"   {metric_name}: {metric_value}")
else:
            print(f"\n‚ö†Ô∏è  No monitoring metrics available yet")
            print(f"   This may take some time to populate after logging events")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Endpoint analysis failed: {e}")
        return False

endpoint_analyzed = analyze_endpoint_metrics()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Query Monitoring Data with SQL

# COMMAND ----------

def query_monitoring_data():
    """Query monitoring data using Databricks SQL."""
    
    print("üîç Querying monitoring data...")
    
    try:
        # Example SQL queries for monitoring data
        # Note: These are example queries - actual table names may vary
        
        queries = {
            "recent_requests": f"""
                SELECT *
                FROM {CATALOG}.{SCHEMA}.serving_requests
                WHERE endpoint_name = '{ENDPOINT_NAME}'
                  AND request_timestamp > now() - INTERVAL 1 DAY
                ORDER BY request_timestamp DESC
                LIMIT 100
            """,
            
            "drift_metrics": f"""
                SELECT *
                FROM {CATALOG}.{SCHEMA}.serving_drift_metrics
                WHERE endpoint_name = '{ENDPOINT_NAME}'
                  AND timestamp > now() - INTERVAL 7 DAY
                ORDER BY timestamp DESC
            """,
            
            "performance_summary": f"""
                SELECT
                    date_trunc('hour', request_timestamp) AS hour,
                    COUNT(*) AS total_requests,
                    SUM(CASE WHEN status_code != 200 THEN 1 ELSE 0 END) AS errors,
                    AVG(latency_ms) AS avg_latency,
                    AVG(CASE WHEN status_code = 200 THEN latency_ms END) AS avg_success_latency
                FROM {CATALOG}.{SCHEMA}.serving_requests
                WHERE endpoint_name = '{ENDPOINT_NAME}'
                  AND request_timestamp > now() - INTERVAL 1 DAY
                GROUP BY hour
                ORDER BY hour DESC
            """
        }
        
        print("üìã Available monitoring queries:")
        for query_name, query in queries.items():
            print(f"   {query_name}: {query.split()[0]}...")
        
        # Execute a sample query (if tables exist)
        try:
            # This would execute the query in a real environment
            print(f"\n‚úÖ Sample query executed successfully")
            print(f"   Query: Recent requests for {ENDPOINT_NAME}")
            print(f"   Note: Results would be displayed in a real environment")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Query execution failed (tables may not exist yet): {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Monitoring data query failed: {e}")
        return False

monitoring_data_queried = query_monitoring_data()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Set Up Alerting for Drift and Failures

# MAGIC %md
# MAGIC ### Create Monitoring Alerts

# COMMAND ----------

def setup_monitoring_alerts():
    """Set up alerts for model monitoring."""
    
    print("üö® Setting up monitoring alerts...")
    
    try:
        # Create alert configurations
        alerts = {
            "high_drift": {
                "name": f"{ENDPOINT_NAME}_high_drift",
                "condition": "drift_score > 0.2",
                "message": f"High drift detected for endpoint {ENDPOINT_NAME}",
                "severity": "high"
            },
            "high_error_rate": {
                "name": f"{ENDPOINT_NAME}_high_error_rate",
                "condition": "error_rate > 0.05",
                "message": f"High error rate detected for endpoint {ENDPOINT_NAME}",
                "severity": "critical"
            },
            "high_latency": {
                "name": f"{ENDPOINT_NAME}_high_latency",
                "condition": "avg_latency > 2000",
                "message": f"High latency detected for endpoint {ENDPOINT_NAME}",
                "severity": "medium"
            },
            "endpoint_down": {
                "name": f"{ENDPOINT_NAME}_down",
                "condition": "endpoint_status != 'ready'",
                "message": f"Endpoint {ENDPOINT_NAME} is down",
                "severity": "critical"
            }
        }
        
        print("‚úÖ Alert configurations created:")
        for alert_name, alert_config in alerts.items():
            print(f"   {alert_name}: {alert_config['condition']} ({alert_config['severity']})")
        
        # Create SQL-based alerts
        alert_queries = {
            "drift_alert": f"""
                SELECT 
                    endpoint_name,
                    AVG(drift_score) as avg_drift
                FROM {CATALOG}.{SCHEMA}.serving_drift_metrics
                WHERE endpoint_name = '{ENDPOINT_NAME}'
                  AND timestamp > now() - INTERVAL 1 HOUR
                GROUP BY endpoint_name
                HAVING AVG(drift_score) > 0.2
            """,
            
            "error_rate_alert": f"""
                SELECT 
                    endpoint_name,
                    COUNT(*) as total_requests,
                    SUM(CASE WHEN status_code != 200 THEN 1 ELSE 0 END) as errors,
                    CAST(SUM(CASE WHEN status_code != 200 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) as error_rate
                FROM {CATALOG}.{SCHEMA}.serving_requests
                WHERE endpoint_name = '{ENDPOINT_NAME}'
                  AND request_timestamp > now() - INTERVAL 1 HOUR
                GROUP BY endpoint_name
                HAVING error_rate > 0.05
            """,
            
            "latency_alert": f"""
                SELECT 
                    endpoint_name,
                    AVG(latency_ms) as avg_latency
                FROM {CATALOG}.{SCHEMA}.serving_requests
                WHERE endpoint_name = '{ENDPOINT_NAME}'
                  AND request_timestamp > now() - INTERVAL 1 HOUR
                  AND status_code = 200
                GROUP BY endpoint_name
                HAVING AVG(latency_ms) > 2000
            """
        }
        
        print(f"\nüìã Alert queries configured:")
        for alert_name, query in alert_queries.items():
            print(f"   {alert_name}: {query.split()[0]}...")
        
        # Save alert configurations
        alert_config_path = f"{MONITORING_RESULTS_DIR}/alert_configurations.json"
        with open(alert_config_path, 'w') as f:
            json.dump({
                'alerts': alerts,
                'queries': alert_queries,
                'endpoint_name': ENDPOINT_NAME,
                'created_date': datetime.now().isoformat()
            }, f, indent=2)
        
        print(f"‚úÖ Alert configurations saved to: {alert_config_path}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Alert setup failed: {e}")
        return False

alerts_configured = setup_monitoring_alerts()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Generate Monitoring Reports

# COMMAND ----------

# MAGIC %md
# MAGIC ### Create Monitoring Dashboard

# COMMAND ----------

def create_monitoring_dashboard():
    """Create a monitoring dashboard configuration."""
    
    print("üìä Creating monitoring dashboard...")
    
    try:
        # Dashboard configuration
        dashboard_config = {
            "dashboard_name": f"{ENDPOINT_NAME}_monitoring",
            "description": f"Monitoring dashboard for {ENDPOINT_NAME}",
            "widgets": [
                {
                    "name": "Request Volume",
                    "type": "line_chart",
                    "query": f"""
                        SELECT 
                            date_trunc('hour', request_timestamp) as hour,
                            COUNT(*) as requests
                        FROM {CATALOG}.{SCHEMA}.serving_requests
                        WHERE endpoint_name = '{ENDPOINT_NAME}'
                          AND request_timestamp > now() - INTERVAL 24 HOUR
                        GROUP BY hour
                        ORDER BY hour
                    """
                },
                {
                    "name": "Error Rate",
                    "type": "line_chart",
                    "query": f"""
                        SELECT 
                            date_trunc('hour', request_timestamp) as hour,
                            CAST(SUM(CASE WHEN status_code != 200 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) as error_rate
                        FROM {CATALOG}.{SCHEMA}.serving_requests
                        WHERE endpoint_name = '{ENDPOINT_NAME}'
                          AND request_timestamp > now() - INTERVAL 24 HOUR
                        GROUP BY hour
                        ORDER BY hour
                    """
                },
                {
                    "name": "Average Latency",
                    "type": "line_chart",
                    "query": f"""
                        SELECT 
                            date_trunc('hour', request_timestamp) as hour,
                            AVG(latency_ms) as avg_latency
                        FROM {CATALOG}.{SCHEMA}.serving_requests
                        WHERE endpoint_name = '{ENDPOINT_NAME}'
                          AND request_timestamp > now() - INTERVAL 24 HOUR
                          AND status_code = 200
                        GROUP BY hour
                        ORDER BY hour
                    """
                },
                {
                    "name": "Drift Score",
                    "type": "line_chart",
                    "query": f"""
                        SELECT 
                            date_trunc('hour', timestamp) as hour,
                            AVG(drift_score) as avg_drift
                        FROM {CATALOG}.{SCHEMA}.serving_drift_metrics
                        WHERE endpoint_name = '{ENDPOINT_NAME}'
                          AND timestamp > now() - INTERVAL 24 HOUR
                        GROUP BY hour
                        ORDER BY hour
                    """
                }
            ],
            "refresh_interval": 300,  # 5 minutes
            "created_date": datetime.now().isoformat()
        }
        
        # Save dashboard configuration
        dashboard_path = f"{MONITORING_RESULTS_DIR}/dashboard_config.json"
        with open(dashboard_path, 'w') as f:
            json.dump(dashboard_config, f, indent=2)
        
        print(f"‚úÖ Dashboard configuration created:")
        print(f"   Dashboard: {dashboard_config['dashboard_name']}")
        print(f"   Widgets: {len(dashboard_config['widgets'])}")
        print(f"   Refresh interval: {dashboard_config['refresh_interval']} seconds")
        print(f"   Configuration saved to: {dashboard_path}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Dashboard creation failed: {e}")
        return False

dashboard_created = create_monitoring_dashboard()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Generate Monitoring Report

# COMMAND ----------

def generate_monitoring_report():
    """Generate a comprehensive monitoring report."""
    
    print("üìã Generating monitoring report...")
    
    try:
        # Create monitoring report
        report = {
            "report_info": {
                "title": f"Model Monitoring Report - {ENDPOINT_NAME}",
                "generated_date": datetime.now().isoformat(),
                "endpoint_name": ENDPOINT_NAME,
                "model_name": MODEL_NAME
            },
            "monitoring_status": {
                "monitoring_enabled": monitoring_enabled,
                "prediction_events_logged": prediction_events_logged,
                "ground_truth_logged": ground_truth_logged,
                "endpoint_analyzed": endpoint_analyzed,
                "alerts_configured": alerts_configured,
                "dashboard_created": dashboard_created
            },
            "configuration": {
                "catalog": CATALOG,
                "schema": SCHEMA,
                "endpoint_name": ENDPOINT_NAME,
                "drift_threshold": 0.1,
                "performance_threshold": 0.95,
                "latency_threshold": 1000
            },
            "recommendations": [
                "Monitor drift metrics daily",
                "Set up automated alerts for critical issues",
                "Review performance metrics weekly",
                "Update ground truth data regularly",
                "Scale endpoint based on traffic patterns"
            ],
            "next_steps": [
                "Implement automated retraining triggers",
                "Set up A/B testing for model updates",
                "Configure external alerting (Slack, email)",
                "Create custom metrics for business KPIs",
                "Plan model versioning strategy"
            ]
        }
        
        # Save report
        report_path = f"{MONITORING_RESULTS_DIR}/monitoring_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"‚úÖ Monitoring report generated:")
        print(f"   Report saved to: {report_path}")
        print(f"   Monitoring status: {'‚úÖ Enabled' if monitoring_enabled else '‚ùå Disabled'}")
        print(f"   Alerts configured: {'‚úÖ Yes' if alerts_configured else '‚ùå No'}")
        print(f"   Dashboard created: {'‚úÖ Yes' if dashboard_created else '‚ùå No'}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Report generation failed: {e}")
        return False

report_generated = generate_monitoring_report()

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. Summary and Next Steps

# COMMAND ----------

print("=" * 60)
print("MODEL MONITORING SUMMARY")
print("=" * 60)

print(f"‚úÖ Monitoring Setup:")
print(f"   Endpoint: {ENDPOINT_NAME}")
print(f"   Model: {MODEL_NAME}")
print(f"   Monitoring enabled: {'‚úÖ' if monitoring_enabled else '‚ùå'}")

print(f"\nüìä Data Logging:")
print(f"   Prediction events: {'‚úÖ Logged' if prediction_events_logged else '‚ùå Failed'}")
print(f"   Ground truth events: {'‚úÖ Logged' if ground_truth_logged else '‚ùå Failed'}")

print(f"\nüîç Analysis:")
print(f"   Endpoint analyzed: {'‚úÖ' if endpoint_analyzed else '‚ùå'}")
print(f"   Monitoring data queried: {'‚úÖ' if monitoring_data_queried else '‚ùå'}")

print(f"\nüö® Alerting:")
print(f"   Alerts configured: {'‚úÖ' if alerts_configured else '‚ùå'}")

print(f"\nüìä Reporting:")
print(f"   Dashboard created: {'‚úÖ' if dashboard_created else '‚ùå'}")
print(f"   Report generated: {'‚úÖ' if report_generated else '‚ùå'}")

print(f"\nüìÅ Results saved to:")
print(f"   Monitoring results: {MONITORING_RESULTS_DIR}")
print(f"   Alert configurations: {MONITORING_RESULTS_DIR}/alert_configurations.json")
print(f"   Dashboard config: {MONITORING_RESULTS_DIR}/dashboard_config.json")
print(f"   Monitoring report: {MONITORING_RESULTS_DIR}/monitoring_report.json")

print(f"\nüéØ Next steps:")
print(f"   1. Monitor the dashboard regularly")
print(f"   2. Respond to alerts promptly")
print(f"   3. Update ground truth data")
print(f"   4. Plan model retraining based on drift")
print(f"   5. Scale monitoring as needed")

print("=" * 60) 